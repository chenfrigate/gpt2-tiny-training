import torch
import os
from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, AutoTokenizer, DataCollatorForLanguageModeling
from datasets import load_dataset

# âœ… å¼ºåˆ¶ä½¿ç”¨ GPU
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # è®© GPU å‚ä¸è®¡ç®—
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")  # æ£€æŸ¥ GPU

# âœ… 2. åŠ è½½ GPT-2 Tiny é…ç½®
config = GPT2Config(
    vocab_size=50257, n_positions=512, n_embd=128, n_layer=4, n_head=4
)
model = GPT2LMHeadModel(config).to(device)  # ğŸš€ ç¡®ä¿æ¨¡å‹åœ¨ GPU

# âœ… å¯ç”¨ Gradient Checkpointingï¼ˆå‡å°‘æ˜¾å­˜å ç”¨ï¼Œå…è®¸æ›´å¤§ batch_sizeï¼‰
model.gradient_checkpointing_enable()

# âœ… 3. åŠ è½½æ•°æ®é›†ï¼ˆå‡å°‘æ•°æ®é‡ï¼ŒåŠ å¿«è®­ç»ƒï¼‰
dataset = load_dataset("wikitext", "wikitext-103-raw-v1", split="train[:10%]")  # åªç”¨ 10% æ•°æ®ï¼ŒåŠ é€Ÿè®­ç»ƒ
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# âœ… è§£å†³ padding é—®é¢˜
tokenizer.pad_token = tokenizer.eos_token  # ä½¿ç”¨ EOS ä½œä¸º padding

# âœ… 4. Tokenizationï¼Œä¼˜åŒ– batch åŠ è½½
def tokenize_function(examples):
    tokens = tokenizer(examples['text'], truncation=True, padding="max_length", max_length=256)  # ğŸš€ é™ä½ max_length
    tokens["labels"] = tokens["input_ids"].copy()  # è®© labels å’Œ input_ids ä¸€è‡´
    return tokens

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])  # ç¡®ä¿ labels è¢«ä¼ å…¥

# âœ… ä½¿ç”¨ DataCollatorForLanguageModelingï¼Œæ”¯æŒä¸åŒ batch size
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# âœ… 5. è®­ç»ƒå‚æ•°ä¼˜åŒ–ï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰
training_args = TrainingArguments(
    output_dir="./models/checkpoint",
    per_device_train_batch_size=16,  # ğŸš€ æé«˜ batch_sizeï¼ŒåŠ å¿«è®­ç»ƒ
    gradient_accumulation_steps=4,  # ğŸš€ ç´¯ç§¯æ¢¯åº¦ï¼Œæ¨¡æ‹Ÿæ›´å¤§ batch
    num_train_epochs=1,  # ğŸš€ é™ä½ epochsï¼Œåªè®­ç»ƒ 1 è½®
    save_steps=5000,  # ğŸš€ é™ä½ checkpoint é¢‘ç‡
    logging_steps=500,
    evaluation_strategy="no",
    learning_rate=5e-4,
    weight_decay=0.01,
    fp16=True,  # ğŸš€ å¯ç”¨æ··åˆç²¾åº¦ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
    save_total_limit=1,  # åªä¿ç•™æœ€è¿‘çš„ checkpointï¼ŒèŠ‚çœç£ç›˜
)

# âœ… 6. è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator  # ğŸš€ ç¡®ä¿ batch é€‚åº”ä¸åŒé•¿åº¦
)

trainer.train()

# âœ… 7. é‡Šæ”¾å†…å­˜ï¼Œé˜²æ­¢ Colab å´©æºƒ
torch.cuda.empty_cache()
import gc
gc.collect()
