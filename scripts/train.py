import torch
import os
from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, AutoTokenizer, DataCollatorForLanguageModeling
from datasets import load_dataset

# âœ… å¼ºåˆ¶ä½¿ç”¨ GPU
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # è®© GPU å‚ä¸è®¡ç®—
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")  # æ£€æŸ¥ GPU

# âœ… 1. åŠ è½½ GPT-2 Tiny é…ç½®
config = GPT2Config(
    vocab_size=50257, n_positions=512, n_embd=128, n_layer=4, n_head=4
)
model = GPT2LMHeadModel(config).to(device)  # ğŸš€ ç¡®ä¿æ¨¡å‹åœ¨ GPU

# âœ… 2. å¯ç”¨ Gradient Checkpointingï¼ˆå‡å°‘æ˜¾å­˜å ç”¨ï¼Œå…è®¸æ›´å¤§ batch_sizeï¼‰
use_gradient_checkpointing = False  # ğŸš€ è®¾ä¸º False ä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼ˆå¦‚æœæ˜¾å­˜è¶³å¤Ÿï¼‰
if use_gradient_checkpointing:
    model.gradient_checkpointing_enable()
else:
    model.gradient_checkpointing_disable()

# âœ… 3. åŠ è½½æ•°æ®é›†ï¼ˆå¢åŠ æ•°æ®é‡ï¼Œæé«˜è®­ç»ƒæ•ˆæœï¼‰
dataset = load_dataset("wikitext", "wikitext-103-raw-v1", split="train[:80%]")  # ğŸš€ å¢åŠ æ•°æ®é‡
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# âœ… 4. è§£å†³ padding é—®é¢˜
tokenizer.pad_token = tokenizer.eos_token  # ä½¿ç”¨ EOS ä½œä¸º padding

# âœ… 5. Tokenizationï¼Œä¼˜åŒ– batch åŠ è½½
def tokenize_function(examples):
    tokens = tokenizer(examples['text'], truncation=True, padding="max_length", max_length=256)
    tokens["labels"] = tokens["input_ids"].copy()  # âœ… è®© labels å’Œ input_ids ä¸€è‡´
    return tokens

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# âœ… 6. ä½¿ç”¨ DataCollatorForLanguageModelingï¼Œæ”¯æŒä¸åŒ batch size
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# âœ… 7. è®­ç»ƒå‚æ•°ä¼˜åŒ–ï¼ˆç¡®ä¿å¯æ¢å¤ï¼‰
training_args = TrainingArguments(
    output_dir="./models/checkpoint",
    per_device_train_batch_size=32,  # ğŸš€ æé«˜ batch_sizeï¼ŒåŠ å¿«è®­ç»ƒ
    gradient_accumulation_steps=1,  # ğŸš€ ç´¯ç§¯æ¢¯åº¦ï¼Œæ¨¡æ‹Ÿæ›´å¤§ batch
    num_train_epochs=3,  # ğŸš€ è®­ç»ƒ 3 è½®
    save_steps=1000,  # âœ… å¢åŠ  checkpoint é¢‘ç‡ï¼Œé˜²æ­¢è®­ç»ƒä¸¢å¤±
    logging_steps=500,
    evaluation_strategy="no",  # âœ… å…³é—­è‡ªåŠ¨è¯„ä¼°
    eval_steps=None,  # âœ… é¿å… Trainer è¯¯è°ƒç”¨ eval
    learning_rate=2e-4,
    weight_decay=0.01,
    fp16=True,  # ğŸš€ å¯ç”¨æ··åˆç²¾åº¦ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
    save_total_limit=2,  # âœ… åªä¿ç•™æœ€è¿‘ 2 ä¸ª checkpointï¼Œé˜²æ­¢å ç”¨ç£ç›˜
    save_strategy="steps",  # âœ… ä»ç„¶æŒ‰ `steps` æ–¹å¼ä¿å­˜ checkpoint
    load_best_model_at_end=False,  # âœ… å…³é—­æœ€ä¼˜æ¨¡å‹åŠ è½½
)

# âœ… 8. æ£€æŸ¥æ˜¯å¦æœ‰ `checkpoint` è¿›è¡Œæ¢å¤è®­ç»ƒ
checkpoint_dir = "./models/checkpoint"
resume_from_checkpoint = None

if os.path.exists(checkpoint_dir) and len(os.listdir(checkpoint_dir)) > 0:
    latest_checkpoint = max(
        [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir) if "checkpoint-" in d],
        key=os.path.getmtime
    )
    resume_from_checkpoint = latest_checkpoint
    print(f"Resuming from checkpoint: {resume_from_checkpoint}")
else:
    print("No checkpoint found, starting fresh training...")

# âœ… 9. è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)

trainer.train(resume_from_checkpoint=resume_from_checkpoint)  # âœ… è‡ªåŠ¨æ¢å¤

# âœ… 10. é‡Šæ”¾å†…å­˜ï¼Œé˜²æ­¢ Colab å´©æºƒ
torch.cuda.empty_cache()
import gc
gc.collect()
