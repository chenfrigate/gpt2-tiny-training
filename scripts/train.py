import torch
from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments, AutoTokenizer, DataCollatorWithPadding
from datasets import load_dataset

# ğŸš€ 1. ç¡®ä¿ GPU è®¾å¤‡å¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")  # è¾“å‡º GPU ä¿¡æ¯

# ğŸš€ 2. åŠ è½½ GPT-2 Tiny é…ç½®
config = GPT2Config(
    vocab_size=50257, n_positions=512, n_embd=128, n_layer=4, n_head=4
)
model = GPT2LMHeadModel(config).to(device)  # ğŸ”¥ å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU

# ğŸš€ 3. åŠ è½½æ•°æ®é›†
dataset = load_dataset("wikitext", "wikitext-103-raw-v1")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# âœ… è§£å†³ padding é—®é¢˜
tokenizer.pad_token = tokenizer.eos_token  # ä½¿ç”¨ EOS ä½œä¸º padding

# ğŸš€ 4. Tokenization å¹¶ç¡®ä¿æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´
def tokenize_function(examples):
    return tokenizer(examples['text'], 
                     truncation=True, 
                     padding="max_length",  # âœ… ç»Ÿä¸€å¡«å……åˆ° max_length
                     max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# âœ… ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼ˆè½¬æ¢ä¸º PyTorch å¼ é‡ï¼‰
tokenized_dataset.set_format(type="torch", columns=["input_ids", "attention_mask"])
tokenized_dataset = tokenized_dataset["train"]  # åªå–è®­ç»ƒé›†

# âœ… ä½¿ç”¨ DataCollator è®© batch é€‚åº”ä¸åŒé•¿åº¦
data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)

# ğŸš€ 5. è®­ç»ƒå‚æ•°ï¼Œä¼˜åŒ–æ˜¾å­˜å ç”¨
training_args = TrainingArguments(
    output_dir="./models/checkpoint",
    per_device_train_batch_size=4,  # ğŸ”¥ é™ä½ batch_size é¿å… OOM
    num_train_epochs=3,
    save_steps=1000,
    logging_steps=500,
    evaluation_strategy="no",
    learning_rate=5e-4,
    weight_decay=0.01,
    fp16=True,  # ğŸ”¥ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ŒåŠ é€Ÿå¹¶é™ä½æ˜¾å­˜å ç”¨
    save_total_limit=2
)

# ğŸš€ 6. è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator  # âœ… è®© batch é€‚åº”ä¸åŒé•¿åº¦
)

trainer.train()

# ğŸš€ 7. é‡Šæ”¾å†…å­˜ï¼Œé˜²æ­¢ Colab å´©æºƒ
torch.cuda.empty_cache()
import gc
gc.collect()
